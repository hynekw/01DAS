{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet, ElasticNetCV\n",
    "from sklearn.metrics import SCORERS, mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "pd.set_option(\"display.max_columns\", 999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Regression techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression problem\n",
    "\n",
    "#### History :)\n",
    "The term \"regression\" was coined by Francis Galton in the nineteenth century to describe a biological phenomenon. The phenomenon was that the heights of descendants of tall ancestors tend to regress down towards a normal average (a phenomenon also known as regression toward the mean).\n",
    "\n",
    "#### Definition\n",
    "Regression predictive modeling is the task of approximating a mapping function from input variables to a numerical output variable.\n",
    "\n",
    "#### Most common regression models\n",
    "* Linear Regression\n",
    "    * Lasso\n",
    "    * Ridge\n",
    "    * ElasticNet\n",
    "* Generalized Linear Regression\n",
    "    * Logistic\n",
    "    * Poisson\n",
    "* Decision Trees\n",
    "* Ensemble methods\n",
    "    * Random Forests\n",
    "    * Gradient Boosting\n",
    "* Neural Networks\n",
    "* Bayesian Regression\n",
    "* SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Variance vs. High Bias problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Bias\n",
    "Bias is the algorithm’s tendency to consistently learn the wrong thing by not taking into account all the information in the data (underfitting)\n",
    "\n",
    "#### Proposals\n",
    "* Try more complex model.\n",
    "* Add features.\n",
    "\n",
    "## High Variance\n",
    "Variance is the algorithm’s tendency to learn random things irrespective of the real signal by fitting highly flexible models that follow the error/noise in the data too closely (overfitting).\n",
    "\n",
    "#### Proposals\n",
    "* Try simpler model.\n",
    "* Remove / reduce some features.\n",
    "* Add regularization parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"img/Bias-Variance-Tradeoff-In-Machine-Learning.png\" style=\"height:600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ElasticNet Model a.k.a. From LM to ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "When we have lots of features, we want to be able to penalize their size and number and thus balance the Bias/Variance problem. \n",
    "\n",
    "Ideally we want to do this by some neat parametrization.\n",
    "\n",
    "\n",
    "### Regularized loss function !\n",
    "\n",
    "$L(\\hat{y}, y)_{reg} = L(\\hat{y}, y) + \\lambda * R(\\beta)$.\n",
    "\n",
    "##### The loss function for ElasticNet regression is\n",
    "$L(\\hat{y}, y)_{lasso} = \\sum_{p}{(\\hat{y}-y)^2} + \\lambda * \\big[(1-\\alpha)/2\\sum_{p}{|\\beta|} + \\alpha \\sum_{p}{\\beta ^2}\\big]$, where $\\lambda \\in N, \\alpha \\in [0, 1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/lasso_vs_ridge.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But how is $\\lambda$ set? => CrossValidation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/cv_mse.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Root MSE function\n",
    "def rmse(x, y):\n",
    "    return sqrt(mean_squared_error(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset into a Pandas dataframe\n",
    "df = pd.read_csv('data/attrition.csv')\n",
    "\n",
    "# drop columns with \n",
    "df.drop(['EmployeeNumber', 'Attrition', 'Over18', 'StandardHours', 'EmployeeCount'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'BusinessTravel', 'DailyRate', 'Department', 'DistanceFromHome',\n",
       "       'Education', 'EducationField', 'EnvironmentSatisfaction', 'Gender',\n",
       "       'HourlyRate', 'JobInvolvement', 'JobLevel', 'JobRole',\n",
       "       'JobSatisfaction', 'MaritalStatus', 'MonthlyIncome', 'MonthlyRate',\n",
       "       'NumCompaniesWorked', 'OverTime', 'PercentSalaryHike',\n",
       "       'PerformanceRating', 'RelationshipSatisfaction', 'StockOptionLevel',\n",
       "       'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance',\n",
       "       'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion',\n",
       "       'YearsWithCurrManager'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what columns fo we have\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets have a look at a scatter matrix plot\n",
    "inspect_cols = ['MonthlyIncome','EducationField',\n",
    "       'JobLevel', 'StockOptionLevel', 'TotalWorkingYears',\n",
    "       'TrainingTimesLastYear', 'WorkLifeBalance', 'YearsAtCompany',\n",
    "       'YearsInCurrentRole', 'YearsSinceLastPromotion',\n",
    "       'YearsWithCurrManager']\n",
    "\n",
    "# fig = px.scatter_matrix(df[inspect_cols])\n",
    "# # increase the resolution\n",
    "# fig.update_layout(\n",
    "#     autosize=False,\n",
    "#     width=2*1024,\n",
    "#     height=2*720,\n",
    "#     paper_bgcolor=\"LightSteelBlue\",\n",
    "# )\n",
    "# #plot the scatter matrix plot\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['JobLevel'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-22e40eef6cd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# lets make it some fun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'JobLevel'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/visual_studio_projects/01DAS_teachers/lectures/lecture_06/.venv_with_catboost/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4115\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4116\u001b[0m             \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4117\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4118\u001b[0m         )\n\u001b[1;32m   4119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/visual_studio_projects/01DAS_teachers/lectures/lecture_06/.venv_with_catboost/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3912\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3913\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3914\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/visual_studio_projects/01DAS_teachers/lectures/lecture_06/.venv_with_catboost/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3944\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3945\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3946\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3947\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/visual_studio_projects/01DAS_teachers/lectures/lecture_06/.venv_with_catboost/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   5338\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5339\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5340\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} not found in axis\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5341\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5342\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['JobLevel'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# lets make it some fun\n",
    "df.drop('JobLevel', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select response variable and features\n",
    "target_col_name = 'MonthlyIncome'\n",
    "num_feature_cols = [\n",
    "        'Age', 'DailyRate','DistanceFromHome', 'Education',\n",
    "        'HourlyRate', 'EnvironmentSatisfaction', 'JobInvolvement',\n",
    "        'JobSatisfaction', 'NumCompaniesWorked', 'PercentSalaryHike',\n",
    "        'RelationshipSatisfaction', 'StockOptionLevel', 'PerformanceRating',\n",
    "        'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance',\n",
    "        'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion',\n",
    "        'YearsWithCurrManager', 'MonthlyRate']\n",
    "cat_feature_cols = [x for x in df.columns if x not in num_feature_cols and x not in [target_col_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast numerical columns as float\n",
    "for col in num_feature_cols:\n",
    "    df[col] = df[col].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create target array and numeric features dataframe\n",
    "df_target = np.ravel(df[[target_col_name]])\n",
    "df_features = df[num_feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataframe to train and test parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_features, df_target, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model rmse: 2986.4758949208863 \n",
      "\n",
      "Model coefs: [-20, 0, -7, 64, -2, -118, 19, 0, -29, 41, 36, 85, -810, 465, -49, 137, 82, -54, 33, -81, 0]\n"
     ]
    }
   ],
   "source": [
    "# fit simple linear regression\n",
    "m0 = LinearRegression().fit(X_train, y_train)\n",
    "m0_rmse = rmse(m0.predict(X_test), y_test)\n",
    "\n",
    "print(f\"Model rmse: {m0_rmse} \\n\")\n",
    "print(f\"Model coefs: {[int(x) for x in m0.coef_]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model rmse: 2972.4704466171906 \n",
      "\n",
      "Model coefs: [-14, 0, -7, 47, -2, -80, 9, 1, -21, -12, 23, 47, -77, 454, -36, 66, 81, -49, 27, -71, 0] \n",
      "\n",
      "Model hyper_params: {'alpha': 1, 'l1_ratio': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# lets try elastic net\n",
    "m1 = ElasticNet(alpha=1, l1_ratio=0.5).fit(X_train, y_train)\n",
    "m1_rmse = rmse(m1.predict(X_test), y_test)\n",
    "hyper_pars = {'alpha': m1.alpha, 'l1_ratio': m1.l1_ratio}\n",
    "\n",
    "\n",
    "print(f\"Model rmse: {m1_rmse} \\n\")\n",
    "print(f\"Model coefs: {[int(x) for x in m1.coef_]} \\n\")\n",
    "print(f\"Model hyper_params: {hyper_pars}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation - hyperparameter tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/kfold.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch CV Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/grid_search_workflow.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mse: 2988.27183682534 \n",
      "\n",
      "Model coefs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 441, 0, 0, 16, 0, 0, 0, 0] \n",
      "\n",
      "Model hyper_params: {'alpha': 841, 'l1_ratio': 1}\n"
     ]
    }
   ],
   "source": [
    "# gridsearch through l1_ratio and alpha parameters grid\n",
    "param_grid = {'l1_ratio': [.1, .5, .7, .9, .95, 1], 'alpha':[x**2 for x in range(30) if x > 0]}\n",
    "m2 = GridSearchCV(estimator=ElasticNet(), param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', iid=False).fit(X_train, y_train)\n",
    "\n",
    "m2_mse = rmse(m2.predict(X_test), y_test)\n",
    "\n",
    "print(f\"Model mse: {m2_mse} \\n\")\n",
    "print(f\"Model coefs: {[int(x) for x in m2.best_estimator_.coef_]} \\n\")\n",
    "print(f\"Model hyper_params: {m2.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Age': -0.0,\n",
       " 'DailyRate': -0.050895142340134335,\n",
       " 'DistanceFromHome': -0.0,\n",
       " 'Education': 0.0,\n",
       " 'HourlyRate': -0.0,\n",
       " 'EnvironmentSatisfaction': -0.0,\n",
       " 'JobInvolvement': -0.0,\n",
       " 'JobSatisfaction': 0.0,\n",
       " 'NumCompaniesWorked': -0.0,\n",
       " 'PercentSalaryHike': -0.0,\n",
       " 'RelationshipSatisfaction': 0.0,\n",
       " 'StockOptionLevel': 0.0,\n",
       " 'PerformanceRating': -0.0,\n",
       " 'TotalWorkingYears': 441.0828861678296,\n",
       " 'TrainingTimesLastYear': -0.0,\n",
       " 'WorkLifeBalance': 0.0,\n",
       " 'YearsAtCompany': 16.007755120757103,\n",
       " 'YearsInCurrentRole': -0.0,\n",
       " 'YearsSinceLastPromotion': 0.0,\n",
       " 'YearsWithCurrManager': -0.0,\n",
       " 'MonthlyRate': 0.023338738149699174}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printout coefs with their names\n",
    "dict(zip(X_train.columns, m2.best_estimator_.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers & Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.k.a. how to do some feture engeneering and not go crazy through CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/pipeline-diagram.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: The pipeline object behaves just like any other estimator object. Which is cool.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OK, lets add some categorical features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create target array and features dataframe\n",
    "add_cat_features = cat_feature_cols\n",
    "df_target = np.ravel(df[[target_col_name]])\n",
    "df_features_all = df[num_feature_cols + add_cat_features]\n",
    "\n",
    "# split the dataframe to train and test parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_features_all, df_target, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mse: 1672.2849470373058 \n",
      "\n",
      "Model coefs: [-78, 31, 46, -384, 490, -105, 451, -97, 86, -212, -157, -69, -31, 31, -555, -2052, -3609, 6904, -460, 6617, -3590, -185, -3068, 26, -72, 45, -9, 9, -6, 0, 4, -40, 0, -34, -121, 45, 0, 28, 32, 31, -597, 202, -47, 112, 45, -33, 39, -46, 0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "column_trans = ColumnTransformer([('onehot', OneHotEncoder(), add_cat_features)], remainder='passthrough')\n",
    "estimators = [('column_trans', column_trans), ('reg', LinearRegression())]\n",
    "\n",
    "pipe0 = Pipeline(estimators)\n",
    "pipe0.fit(X_train, y_train)\n",
    "\n",
    "pipe0_mse = rmse(pipe0.predict(X_test), y_test)\n",
    "\n",
    "print(f\"Model mse: {pipe0_mse} \\n\")\n",
    "print(f\"Model coefs: {[int(x) for x in pipe0.named_steps['reg'].coef_]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mse: 2777.576814341133 \n",
      "\n",
      "Model coefs: [24, -41, 15, -21, -64, 87, 7, 26, 35, -47, 15, -39, 22, -22, -9, -61, -466, 487, 23, 575, -487, 56, -117, 54, 19, -74, 16, -16, -15, 0, -6, 38, -1, -71, 11, 10, -17, -13, 23, 18, -72, 431, -33, 54, 79, -50, 28, -71, 0] \n",
      "\n",
      "Model hyper_params: {'alpha': 1.0, 'l1_ratio': 0.5}\n"
     ]
    }
   ],
   "source": [
    "column_trans = ColumnTransformer([('onehot', OneHotEncoder(dtype='int'), add_cat_features)], remainder='passthrough')\n",
    "estimators = [('column_trans', column_trans), ('reg', ElasticNet())]\n",
    "\n",
    "pipe1 = Pipeline(estimators)\n",
    "pipe1.fit(X_train, y_train)\n",
    "\n",
    "pipe1_mse = rmse(pipe1.predict(X_test), y_test)\n",
    "hyper_pars = {'alpha': pipe1.named_steps['reg'].alpha, 'l1_ratio': pipe1.named_steps['reg'].l1_ratio}\n",
    "\n",
    "print(f\"Model mse: {pipe1_mse} \\n\")\n",
    "print(f\"Model coefs: {[int(x) for x in pipe1.named_steps['reg'].coef_]} \\n\")\n",
    "print(f\"Model hyper_params: {hyper_pars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mse: 1677.0303359673717 \n",
      "\n",
      "Model coefs: [0, 0, 0, -103, 239, 0, 0, 0, 54, -85, 0, 0, -3, 0, 0, -1448, -2941, 7001, 0, 6919, -2910, 53, -2516, 0, -55, 0, 0, 0, -6, 0, 4, -19, 0, -24, -97, 37, 0, 12, 21, 9, -406, 213, -39, 82, 44, -30, 35, -45, 0] \n",
      "\n",
      "Model hyper_params: {'reg__alpha': 10, 'reg__l1_ratio': 1}\n"
     ]
    }
   ],
   "source": [
    "column_trans = ColumnTransformer([('onehot', OneHotEncoder(dtype='int'), add_cat_features)], remainder='passthrough')\n",
    "estimators = [('column_trans', column_trans), ('reg', ElasticNet())]\n",
    "pipe1 = Pipeline(estimators)\n",
    "param_grid = {'reg__l1_ratio': [.1, .5, .7, .9, .95, 1], 'reg__alpha':[0.2, 0.5, 1, 4, 10, 20, 40, 100]}\n",
    "m3 = GridSearchCV(estimator=pipe1, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', iid=False).fit(X_train, y_train)\n",
    "\n",
    "m3_mse = rmse(m3.predict(X_test), y_test)\n",
    "\n",
    "print(f\"Model mse: {m3_mse} \\n\")\n",
    "print(f\"Model coefs: {[int(x) for x in m3.best_estimator_.named_steps['reg'].coef_]} \\n\")\n",
    "print(f\"Model hyper_params: {m3.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mse: 1671.9526541529583 \n",
      "\n",
      "Model coefs: [0, 0, 0, 0, 115, 0, 0, 0, 0, -31, 0, 0, 0, 0, 0, -1336, -2870, 7041, 0, 6900, -2832, 0, -2390, 0, -18, 0, 0, 0, 0, 0, 0, 0, 0, 0, -74, 8, 0, 0, 0, 0, -228, 8143, 0, 0, 90, 0, 255, 0, 0] \n",
      "\n",
      "Model hyper_params: {'reg__alpha': 20, 'reg__l1_ratio': 1}\n"
     ]
    }
   ],
   "source": [
    "# added scaler\n",
    "column_trans = ColumnTransformer([('onehot', OneHotEncoder(dtype='int'), add_cat_features)], remainder=MinMaxScaler())\n",
    "estimators = [('column_trans', column_trans), ('reg', ElasticNet())]\n",
    "pipe1 = Pipeline(estimators)\n",
    "param_grid = {'reg__l1_ratio': [.1, .5, .7, .9, .95, 1], 'reg__alpha':[0.2, 0.5, 1, 4, 10, 20, 40, 100]}\n",
    "m4 = GridSearchCV(estimator=pipe1, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', iid=False).fit(X_train, y_train)\n",
    "\n",
    "m4_mse = rmse(m4.predict(X_test), y_test)\n",
    "\n",
    "print(f\"Model mse: {m4_mse} \\n\")\n",
    "print(f\"Model coefs: {[int(x) for x in m4.best_estimator_.named_steps['reg'].coef_]} \\n\")\n",
    "print(f\"Model hyper_params: {m4.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees - bricks of ensemble models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is it?\n",
    "A model that consists of k nodes with binary decision rule. \n",
    "\n",
    "#### How does it predict?\n",
    "1] Is your weight >= 90 -> True\n",
    "\n",
    "2] Is your level of excersise >= 0 -> False\n",
    "\n",
    "3] Do you have stressful job -> True\n",
    "\n",
    "--> You will die at np.mean([63, 82, 54, 61, 64]) = 64.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How is it trained?\n",
    "\n",
    "1] Fow each candidate split $\\theta$ (across all fetures), at node with $Q$ remaining data points, scoring function $G(Q, \\theta)$ is computed as:\n",
    "\n",
    "$G(Q, \\theta) = \\frac{n_{left}}{N_m} H(Q_{left}(\\theta)) + \\frac{n_{right}}{N_m} H(Q_{right}(\\theta))$ \n",
    "\n",
    "Where $Q_{left}$ represents the left side of the node. The inpurity function $H()$ is defined as MSE for regression task.\n",
    "\n",
    "2] You select $\\theta^* = argmin_{\\theta} G(Q, \\theta)$ as your next split.\n",
    "\n",
    "This is repeated until there is nothing to split or stopping criterion is hit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why is it so bad?\n",
    "The decision trees tend to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/tree_fit.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mse: 1780.9897028884568 \n",
      "\n",
      "Model hyper_params: {'reg__max_depth': 5}\n"
     ]
    }
   ],
   "source": [
    "column_trans = ColumnTransformer([('onehot', OneHotEncoder(dtype='int'), add_cat_features)], remainder=MinMaxScaler())\n",
    "estimators = [('column_trans', column_trans), ('reg', DecisionTreeRegressor())]\n",
    "pipe2 = Pipeline(estimators)\n",
    "param_grid = {'reg__max_depth': [3, 5]}\n",
    "m4 = GridSearchCV(estimator=pipe2, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', iid=False).fit(X_train, y_train)\n",
    "\n",
    "m4_mse = rmse(m4.predict(X_test), y_test)\n",
    "\n",
    "print(f\"Model mse: {m4_mse} \\n\")\n",
    "print(f\"Model hyper_params: {m4.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'img/tree_example.pdf'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz \n",
    "dot_data = export_graphviz(m4.best_estimator_.named_steps['reg'], out_file=None) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"img/tree_example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/tree_example.pdf\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble models - Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/rf.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mse: 1558.9378699145716 \n",
      "\n",
      "Model hyper_params: {'reg__max_depth': 10, 'reg__max_features': 'auto', 'reg__n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "column_trans = ColumnTransformer([('onehot', OneHotEncoder(dtype='int'), add_cat_features)], remainder=MinMaxScaler())\n",
    "estimators = [('column_trans', column_trans), ('reg', RandomForestRegressor())]\n",
    "pipe2 = Pipeline(estimators)\n",
    "param_grid = {'reg__max_depth': [5, 10, 15], 'reg__n_estimators':[40, 100, 150], 'reg__max_features':['auto', 'sqrt']}\n",
    "m4 = GridSearchCV(estimator=pipe2, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', iid=False).fit(X_train, y_train)\n",
    "\n",
    "m4_mse = rmse(m4.predict(X_test), y_test)\n",
    "\n",
    "print(f\"Model mse: {m4_mse} \\n\")\n",
    "print(f\"Model hyper_params: {m4.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble models - Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weak learner -> errors -> Weak learner -> errors -> ...\n",
    "\n",
    "If we define weak learner in $m$-th iteration step as $h_m$ and final model in step $m$ as $F_m$, then the iteration formula is:\n",
    "\n",
    "$F_m(X) = F_{m-1}(X) + \\lambda h_m(X)$,\n",
    "\n",
    "where $X$ are the data and $\\lambda$ is learning raste coefficient. Initial model $F_0$ is just weak learner on the data $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/xgb.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_trans = ColumnTransformer([('onehot', OneHotEncoder(dtype='int'), add_cat_features)], remainder=MinMaxScaler())\n",
    "estimators = [('column_trans', column_trans), ('reg', GradientBoostingRegressor())]\n",
    "pipe2 = Pipeline(estimators)\n",
    "param_grid = {'reg__max_features': ['auto', 'sqrt'], 'reg__subsample': [0.1, 0.05, 0.4], 'reg__min_samples_leaf': [0.0025, 0.005, 0.01, 0.05, 0.1], 'reg__n_estimators':[30, 40, 50, 70, 200]}\n",
    "m4 = GridSearchCV(estimator=pipe2, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', iid=False).fit(X_train, y_train)\n",
    "\n",
    "m4_mse = rmse(m4.predict(X_test), y_test)\n",
    "\n",
    "print(f\"Model mse: {m4_mse} \\n\")\n",
    "print(f\"Model hyper_params: {m4.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 282.7339829\ttotal: 50.2ms\tremaining: 50.2ms\n",
      "1:\tlearn: 282.1288038\ttotal: 52.7ms\tremaining: 0us\n",
      "[446.89976575 546.90499444 585.95009165 546.90499444 638.51599298\n",
      " 546.90499444 591.87394552 446.89976575 446.89976575 585.95009165\n",
      " 446.89976575 446.89976575 446.89976575 638.51599298 546.90499444\n",
      " 446.89976575 471.55549188 638.51599298 446.89976575 546.90499444\n",
      " 446.89976575 446.89976575 546.90499444 546.90499444 585.95009165\n",
      " 446.89976575 446.89976575 471.55549188 585.95009165 446.89976575\n",
      " 446.89976575 446.89976575 546.90499444 546.90499444 638.51599298\n",
      " 446.89976575 485.94486295 491.86871682 446.89976575 446.89976575\n",
      " 446.89976575 546.90499444 471.55549188 446.89976575 638.51599298\n",
      " 585.95009165 446.89976575 446.89976575 446.89976575 446.89976575]\n"
     ]
    }
   ],
   "source": [
    "# initialize data\n",
    "train_data = np.random.randint(0, \n",
    "                               100, \n",
    "                               size=(100, 10))\n",
    "train_label = np.random.randint(0, \n",
    "                                1000, \n",
    "                                size=(100))\n",
    "test_data = np.random.randint(0, \n",
    "                              100, \n",
    "                              size=(50, 10))\n",
    "# initialize Pool\n",
    "train_pool = Pool(train_data, \n",
    "                  train_label, \n",
    "                  cat_features=[0,2,5])\n",
    "test_pool = Pool(test_data, \n",
    "                 cat_features=[0,2,5]) \n",
    "\n",
    "# specify the training parameters \n",
    "model = CatBoostRegressor(iterations=2, \n",
    "                          depth=2, \n",
    "                          learning_rate=1, \n",
    "                          loss_function='RMSE')\n",
    "#train the model\n",
    "model.fit(train_pool)\n",
    "# make the prediction using the resulting model\n",
    "preds = model.predict(test_pool)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BusinessTravel',\n",
       " 'Department',\n",
       " 'EducationField',\n",
       " 'Gender',\n",
       " 'JobRole',\n",
       " 'MaritalStatus',\n",
       " 'OverTime']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 2855.9589244\ttotal: 3.88ms\tremaining: 3.88ms\n",
      "1:\tlearn: 2184.4195187\ttotal: 6.12ms\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2043.2030074055815"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize Pool\n",
    "train_pool = Pool(X_train, \n",
    "                  y_train, \n",
    "                  cat_features=cat_feature_cols)\n",
    "test_pool = Pool(X_test, \n",
    "                 cat_features=cat_feature_cols) \n",
    "\n",
    "# specify the training parameters \n",
    "model = CatBoostRegressor(iterations=2, \n",
    "                          depth=2, \n",
    "                          learning_rate=1, \n",
    "                          loss_function='RMSE')\n",
    "#train the model\n",
    "model.fit(train_pool)\n",
    "\n",
    "\n",
    "m4_mse = rmse(model.predict(test_pool), y_test)\n",
    "m4_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources:\n",
    "[Bias/Variance problem](https://www.learnopencv.com/bias-variance-tradeoff-in-machine-learning/)\n",
    "\n",
    "[Scikit supervized models](https://scikit-learn.org/stable/supervised_learning.html)\n",
    "\n",
    "[Bayessian regression post](https://stats.stackexchange.com/questions/252577/bayes-regression-how-is-it-done-in-comparison-to-standard-regression/252608)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_with_catboost",
   "language": "python",
   "name": ".venv_with_catboost"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
