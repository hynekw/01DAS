{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Statistical Classification:\n",
    "* Machine learning method based on supervised learning.\n",
    "* Categories are predefined (unlike clustering in unsupervised learning).\n",
    "* For two categories, the problem is known as binary classification. \n",
    "* Classifier assigns the most probable class to new observation, given the training set of examples.\n",
    "* Bayes - density based classifiers vs. non density based classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curse of dimensionality\n",
    "\n",
    "* When the dimensionality increases, the volume of the space increases so fast that the available data become sparse. \n",
    "* To obtain a statistically reliable result, the amount of data needed to support the result often grows exponentially with the dimensionality.\n",
    "* In classification, an enormous amount of training data is required to ensure that there are several samples with each combination of values.\n",
    "* The peaking paradox (Hughes phenomenon).\n",
    " \n",
    " ![Peaking](pics/Peaking.png)\n",
    " * Balancing feature size, training set size and the choice of the classifier is a basic problem in the design of classification problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Overtraining \n",
    "* It happens if the complexity of the system we have chosen for training is too large for the given dataset.\n",
    "* The trained system will adapt to the noise instead of to the class differences.\n",
    "* The solution is either to enlarge the dataset, but, if not possible, to simplify the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary vs. Multiple\n",
    "Wiki: Classification can be thought of as two separate problems – binary classification and multiclass classification. In binary classification, a better understood task, only two classes are involved, whereas multiclass classification involves assigning an object to one of several classes.[9] Since many classification methods have been developed specifically for binary classification, multiclass classification often requires the combined use of multiple binary classifiers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear classifier\n",
    "* Linear Discriminant Analysis (or Fisher's linear discriminant) (LDA)—assumes Gaussian conditional density models\n",
    "* Naive Bayes classifier with multinomial or multivariate Bernoulli event models.\n",
    "* Logistic regression—maximum likelihood estimation of w → {\\displaystyle {\\vec {w}}} {\\vec {w}} assuming that the observed training set was generated by a binomial model that depends on the output of the classifier.\n",
    "* Perceptron—an algorithm that attempts to fix all errors encountered in the training set\n",
    "* Support vector machine—an algorithm that maximizes the margin between the decision hyperplane and the examples in the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees,Random forests\n",
    "\n",
    "Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification metrics\n",
    "* Precision, Accuracy, Sensitivity, Specificity\n",
    " \n",
    "![Precisionrecall](pics/Precisionrecall.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision, recall and ROC\n",
    "\n",
    "* true positive  (TP)\n",
    "* true negative  (TN)\n",
    "* false positive (FP)\n",
    "* false negative (FN)\n",
    "\n",
    "* sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "$$  {TPR} ={\\frac { {TP} }{ {P} }}={\\frac { {TP} }{ {TP} + {FN} }}=1- {FNR} $$\n",
    "\n",
    "* specificity, selectivity or true negative rate (TNR)\n",
    "$$ {TNR} ={\\frac { {TN} }{ {N} }}={\\frac { {TN} }{ {TN} + {FP} }}=1- {FPR} $$\n",
    "\n",
    "* precision or positive predictive value (PPV)\n",
    "$${  {PPV} ={\\frac { {TP} }{ {TP} + {FP} }}=1- {FDR} }$$\n",
    "\n",
    "* negative predictive value (NPV)\n",
    "$${  {NPV} ={\\frac { {TN} }{ {TN} + {FN} }}=1- {FOR} } $$\n",
    "\n",
    "* false positive rate (FPR) (alpha - type I error)\n",
    "$$ {  {FPR} ={\\frac { {FP} }{ {N} }}={\\frac { {FP} }{ {FP} + {TN} }}=1- {TNR} } $$\n",
    "\n",
    "* false negative rate (FNR) (beta - type II error)\n",
    "$$ {  {FNR} ={\\frac { {FN} }{ {P} }}={\\frac { {FN} }{ {FN} + {TP} }}=1- {TPR} }$$\n",
    "\n",
    "* acuracy (ACC)\n",
    "$$ {  {ACC} ={\\frac { {TP} + {TN} }{ {P} + {N} }}={\\frac { {TP} + {TN} }{ {TP} + {TN} + {FP} + {FN} }}} $$\n",
    "\n",
    "* F1 score is the harmonic mean of precision and sensitivity\n",
    "$${  {F} _{1}=2\\cdot {\\frac { {PPV} \\cdot  {TPR} }{ {PPV} + {TPR} }}={\\frac {2 {TP} }{2 {TP} + {FP} + {FN} }}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ROC_curves](pics/ROC_curves.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generalization of Recall and Precision to multiclass problems is to sum over rows (columns) of the confusion matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "covered in the last lesson, 10 can be replaced by k\n",
    "* 10-fold cross-validation, very popular and has become a standard procedure in many papers and works.\n",
    "* 10 times 10-fold cross-validation, useful for comparing classifiers as the 10 repeats shrink the standard deviations in the means of the estimated classifier errors.\n",
    "* 10 times 2-fold cross-validation helps compute the significance of differences in classification error means.\n",
    "* Leave-one-out (LOO) cross-validation. In this case the number of folds is equal to the number of objects in the design set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "* Help us to find the \"best\" hyperparameters of given method to optimize loss function or evaluation metrics.\n",
    "* Use pipelines and run the process as much as possible. Depends on time and resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to code ...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our dataset\n",
    "data = 'data/attrition.csv'\n",
    "df = pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=42)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's drop EmployeeID\n",
    "df.drop(['EmployeeNumber', 'EmployeeCount','StandardHours','Over18'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select response variable and features\n",
    "target_col_name = 'Attrition'\n",
    "num_feature_cols = [\n",
    "        'Age', 'DailyRate','DistanceFromHome', 'Education',\n",
    "        'HourlyRate', 'EnvironmentSatisfaction', 'JobInvolvement','JobLevel',\n",
    "        'JobSatisfaction', 'NumCompaniesWorked', 'PercentSalaryHike',\n",
    "        'RelationshipSatisfaction', 'StockOptionLevel', 'PerformanceRating',\n",
    "        'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance',\n",
    "        'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion',\n",
    "        'YearsWithCurrManager', 'MonthlyRate']\n",
    "cat_feature_cols = [x for x in df.columns if x not in num_feature_cols and x not in [target_col_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, hue=\"Attrition\",vars=['Age','DailyRate','DistanceFromHome','YearsAtCompany','YearsSinceLastPromotion'],palette=\"hls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(22,22))\n",
    "sns.heatmap(df.corr(), cmap='Reds',linewidth=.5,annot=True)\n",
    "sns.set(font_scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast numerical columns as float\n",
    "for col in num_feature_cols:\n",
    "    df[col] = df[col].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = np.ravel(df[[target_col_name]])\n",
    "df_features = df[num_feature_cols]\n",
    "df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_features, df_target, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use standard version of logistic regression\n",
    "logmodel=LogisticRegression()\n",
    "logmodel.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred=logmodel.predict(X_test)\n",
    "classification_report(Y_test, Y_pred)\n",
    "target_names = ['No', 'Yes']\n",
    "print(classification_report(Y_test, Y_pred, target_names))\n",
    "# Note that in binary classification, \n",
    "# recall of the positive class is also known as “sensitivity”;\n",
    "# recall of the negative class is “specificity”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(Y_test, Y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(Y_test,Y_pred),annot=True,fmt='3.0f',cmap=\"winter\")\n",
    "plt.title('Confusion matrix', y=1, size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of R style analysis :-D\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy import stats\n",
    "\n",
    "mod1 = smf.glm(formula='Attrition ~Age * DailyRate + DistanceFromHome + Education + HourlyRate', data=df, family=sm.families.Binomial()).fit()\n",
    "mod1.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating K fold Cross-validation \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(logmodel, # model\n",
    "                         X_train, # Feature matrix\n",
    "                         Y_train, # Target vector\n",
    "                         cv=kf, # Cross-validation technique\n",
    "                         scoring=\"accuracy\", # Loss function\n",
    "                         n_jobs=-1) # Use all CPU scores\n",
    "print('10 fold CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC()\n",
    "svc.fit(X_train, Y_train)\n",
    "Y_predSVC = svc.predict(X_test)\n",
    "print(classification_report(Y_test, Y_predSVC, target_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "* [ ] https://karpathy.github.io/neuralnets/\n",
    "* [ ] https://playground.tensorflow.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR using NN\n",
    "Let's predict Exclusive OR using NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "+---+---+---------+\n",
    "| A | B | A XOR B |\n",
    "+---+---+---------+\n",
    "| 1 | 1 | 0       |\n",
    "+---+---+---------+\n",
    "| 1 | 0 | 1       |\n",
    "+---+---+---------+\n",
    "| 0 | 1 | 1       |\n",
    "+---+---+---------+\n",
    "| 0 | 0 | 0       |\n",
    "+---+---+---------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It's a binary classification problem, and a supervised one. Our task is to create a neural network that will predict the values of one logical function given the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume the simple shallow architecture with hidden layer\n",
    "![NNet](pics/NNEt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `X1,X2` = data input\n",
    "* `N1,N2,N3` = neurons\n",
    "* `B1,B2` = bias\n",
    "* `W..` = weights\n",
    "* `b..` = bias weights\n",
    "* `Y` = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias node is \"always on\" -- in NN has the role of the intercept and it's providing a way to get non-zero output on zero inputs. Without bias, the activation of features = 0 would be always zero. When using sigmoid function for example, the output of the (0,0) would be 0.5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll assume sigmoid neurons:\n",
    "* they accept real values as inputs\n",
    "* the value of activation is a dot product of weights and inputs (+ bias), i.e. `W*out_{prev} + b`\n",
    "* output is a sigmoid function of its activation value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    x is assumed to be sigmoid function!\n",
    "    \"\"\"\n",
    "    return x*(1-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-10,10,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, sigmoid(x), x, sigmoid_derivative(sigmoid(x)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning\n",
    "Information is stored in connections between the neurons -- the weights. NN learns by updating its weights according to a learning algorithm that helps it converge to the expected output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initialize the weights and biases randomly.\n",
    "* Iterate over the data\n",
    "    * Compute the predicted output\n",
    "    * Compute the loss (distance from the data)\n",
    "    * `W(new) = W(old) — α ∆W`\n",
    "    * `b(new) = b(old) — α ∆b`\n",
    "* Repeat until the error is minimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update step is assumed in the direction of gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two parts to the learning algorithm\n",
    "* Forward pass (computation of the predicted output)\n",
    "* Backward pass, aka backpropagation (change of the weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data\n",
    "Let's create our learning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y = np.array([[0],[1],[1],[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization of the weights and biases\n",
    "Let's sample normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(n_X,n_H,n_Y):\n",
    "    \"\"\"\n",
    "    n_X ... number of input layer neurons\n",
    "    n_H ... number of hidden layer neurons\n",
    "    n_Y ... number of output layer neurons\n",
    "    \"\"\"\n",
    "    W1 = np.random.randn(n_X, n_H)\n",
    "    b1 = np.zeros((1, n_H))\n",
    "    W2 = np.random.randn(n_H, n_Y)\n",
    "    b2 = np.zeros((1, n_Y))\n",
    "    \n",
    "    return W1,b1,W2,b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(W1, b1, W2, b2) = init_params(2, 2, 1)\n",
    "print(\"Initial hidden weights: \",end='')\n",
    "print(*W1)\n",
    "print(\"Initial hidden biases: \",end='')\n",
    "print(*b1)\n",
    "print(\"Initial output weights: \",end='')\n",
    "print(*W2)\n",
    "print(\"Initial output biases: \",end='')\n",
    "print(*b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10000\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward propagation\n",
    "Computing the predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X,W1,b1,W2,b2):\n",
    "    hidden_layer_activation = np.dot(X,W1) + b1\n",
    "    hidden_layer_output = sigmoid(hidden_layer_activation)\n",
    "\n",
    "    output_layer_activation = np.dot(hidden_layer_output,W2) + b2 \n",
    "    Y_hat = sigmoid(output_layer_activation)\n",
    "    \n",
    "    return hidden_layer_output, Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error (loss) function\n",
    "Let's use mean squared error. Typically used rather for regression problems, but we'll ignore that for now.\n",
    "\n",
    "$E = \\frac{1}{2}(Y - Y_{hat})^2$\n",
    "\n",
    "We're going to need it's derivative (w.r.t. $Y_{hat}$)\n",
    "\n",
    "$\\frac{\\partial E}{\\partial Y_{hat}} = -(Y - Y_{hat})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(Y,Y_hat):\n",
    "    loss = 1/2*np.sum(np.power(Y-Y_hat,2))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_derivative(Y,Y_hat):\n",
    "    d_loss = -(Y - Y_hat)\n",
    "    \n",
    "    return d_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation\n",
    "Goal of the backpropagation is to change the weights in order to minimize the error/loss. Since the outcome is a function of activation and further activation is a function of weights, so we want to know\n",
    "\n",
    "$\\frac{\\partial E}{\\partial w21} = \\frac{\\partial E}{\\partial Y_{hat}} * \\frac{\\partial Y_{hat}}{\\partial net_{N3}} * \\frac{\\partial net_{N3}}{\\partial w21}$\n",
    "\n",
    "\n",
    "We already know the derivative of the error with respect to the prediction. The second term is derivative of the sigmoid\n",
    "\n",
    "$\\frac{\\partial Y_{hat}}{\\partial net_{N3}} = Y_{hat} * (1 - Y_{hat})$\n",
    "\n",
    "and the last term is a derivative of the activation by weights, so output of the hidden layer. \n",
    "\n",
    "$\\frac{\\partial net_{N3}}{\\partial w21} = out_{N1}$\n",
    "\n",
    "The same logic applied to other weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(d_loss, Y_hat, W2, hidden_layer_output):\n",
    "    d_Y_hat = d_loss * sigmoid_derivative(Y_hat)\n",
    "    error_hidden_layer = d_Y_hat.dot(W2.T)\n",
    "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
    "    \n",
    "    return d_Y_hat, d_hidden_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training algorithm\n",
    "for i in range(epochs):\n",
    "    #Forward Propagation\n",
    "    hidden_layer_output, Y_hat = forward_prop(X,W1,b1,W2,b2)\n",
    "\n",
    "    # Compute loss and derivatives\n",
    "    loss = loss_function(Y,Y_hat)\n",
    "    d_loss = loss_derivative(Y,Y_hat)\n",
    "    \n",
    "    #Backpropagation\n",
    "    d_Y_hat, d_hidden_layer = back_prop(d_loss, Y_hat, W2, hidden_layer_output)\n",
    "    \n",
    "    #Updating Weights and Biases (Gradient descent)\n",
    "    W2 = W2 - hidden_layer_output.T.dot(d_Y_hat) * lr\n",
    "    b2 = b2 - np.sum(d_Y_hat,axis=0,keepdims=True) * lr\n",
    "    W1 = W1 - X.T.dot(d_hidden_layer) * lr\n",
    "    b1 = b1 - np.sum(d_hidden_layer,axis=0,keepdims=True) * lr\n",
    "    \n",
    "    if(i%1000 == 0):\n",
    "        print('Loss after iteration# {:d}: {:f}'.format(i, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final hidden weights: \",end='')\n",
    "print(*W1)\n",
    "print(\"Final hidden bias: \",end='')\n",
    "print(*b1)\n",
    "print(\"Final output weights: \",end='')\n",
    "print(*W2)\n",
    "print(\"Final output bias: \",end='')\n",
    "print(*b2)\n",
    "\n",
    "print(\"\\nOutput from neural network after 10,000 epochs: \",end='')\n",
    "print(*Y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatives & Improvements\n",
    "\n",
    "### Loss functions\n",
    "* Has significance on learning/updating\n",
    "* Choose according to the problem\n",
    "    * Regression: MSE, MSLE, MAE\n",
    "    * Binary Classification: Binary Cross-Entropy, Hinge loss,...\n",
    "    * Multi-class Classification: KL-divergence, Multi Cross-Entropy,...\n",
    "    \n",
    "### Activation functions\n",
    "* Linear function\n",
    "    * combinations are linear, not good approximative properties\n",
    "    * unbounded range\n",
    "    * constant learning rate\n",
    "* Sigmoids\n",
    "    * in [0,1]\n",
    "    * non-linear, good for stacking layers\n",
    "    * sensitive around zero\n",
    "    * \"vanishing gradients\" -- near zero learning rate\n",
    "* Tanh\n",
    "    * \"Scaled Sigmoid\", $2*sigmoid(2x)-1$\n",
    "    * steeper gradient\n",
    "* ReLu\n",
    "    * $A(x) = max(x,0)$\n",
    "    * does not fire up all activations as sigmoids -- that's costly in big networks\n",
    "    * combinations are non-linear -- good approximator\n",
    "    * less computationally demanding\n",
    "    * \"dying gradient\" -- neurons will stop responding because the gradient = 0 for negative values\n",
    "    * \"leaky ReLu\" -- $y=0.01x,\\, \\mathrm{for}\\, x<0$, the idea is to gradient recover itself\n",
    "    \n",
    "* Generally, ReLu are the most used one, but sigmoids can be better approximators (if more costly ones)\n",
    "\n",
    "### Learning step\n",
    "* some alternatives to backprop, but mostly minor\n",
    "* https://www.technologyreview.com/s/608911/is-ai-riding-a-one-trick-pony/\n",
    "* many alternatives to gradient descent!\n",
    "    * proximal grads\n",
    "    * evolutionary algorithms\n",
    "    * Ada optimizers\n",
    "    * stochastic gradient descent\n",
    "    \n",
    "    \n",
    "### Architecture\n",
    "* many variants of mostly deep networks\n",
    "* CNNs (use convolution instead of matrix multiplication) in one or more places\n",
    "* RNNs (use recurrence, long short-term memory -- good for time-series data, simulate lags)\n",
    "* more engineering than math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras\n",
    "High-level framework for NNets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our dataset\n",
    "data = 'data/attrition.csv'\n",
    "df = pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's drop EmployeeID\n",
    "df.drop(['EmployeeNumber', 'EmployeeCount'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to input and predicted data\n",
    "Xk = df.loc[:,df.columns!='Attrition']\n",
    "Yk = df.loc[:,'Attrition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're going to reuse some tricks from the last lesson\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's label numerical columns\n",
    "num_feature_cols = [ 'Age', 'DailyRate','DistanceFromHome', 'Education',\n",
    "        'HourlyRate', 'EnvironmentSatisfaction', 'JobInvolvement', 'JobLevel',\n",
    "        'JobSatisfaction', 'MonthlyIncome','NumCompaniesWorked', 'PercentSalaryHike',\n",
    "        'RelationshipSatisfaction', 'StockOptionLevel', 'PerformanceRating', 'StandardHours',\n",
    "        'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance',\n",
    "        'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion',\n",
    "        'YearsWithCurrManager', 'MonthlyRate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and categorical\n",
    "cat_feature_cols = [x for x in Xk.columns if x not in num_feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines!\n",
    "num_transformer = Pipeline(steps=[\n",
    "                  ('imputer', SimpleImputer(strategy='median')),\n",
    "                  ('scaler', RobustScaler())])\n",
    "\n",
    "cat_transformer = Pipeline(steps=[\n",
    "                  ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                  ('onehot', OneHotEncoder(categories='auto', \n",
    "                                     sparse=False, \n",
    "                                     handle_unknown='ignore'))])\n",
    "\n",
    "pipeline_preprocess = ColumnTransformer(transformers=[\n",
    "        ('numerical_preprocessing', num_transformer, num_feature_cols),\n",
    "        ('categorical_preprocessing', cat_transformer, cat_feature_cols)],\n",
    "        remainder='passthrough')\n",
    "\n",
    "pipe0 = Pipeline([(\"transform_inputs\", pipeline_preprocess)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply transformations\n",
    "Xkk = pipe0.fit_transform(Xk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative way to get booleans\n",
    "Ykk = Yk.str.get_dummies().iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xk_train, Xk_test, Yk_train, Yk_test = train_test_split(Xkk, Ykk, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#First Hidden Layer\n",
    "model.add(Dense(4, activation='relu', kernel_initializer='random_normal', input_dim=Xk_train.shape[1]))\n",
    "#Second  Hidden Layer\n",
    "model.add(Dense(4, activation='relu', kernel_initializer='random_normal'))\n",
    "#Output Layer\n",
    "model.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the neural network\n",
    "model.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(Xk_train,Yk_train, batch_size=10, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[loss_train, accuracy_train] = model.evaluate(Xk_train, Yk_train)\n",
    "print('Loss and accuracy on train set {:f}: {:f}'.format(loss_train, accuracy_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's predict on test set\n",
    "Yk_pred = model.predict(Xk_test)\n",
    "Yk_pred = (Yk_pred>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "cm = confusion_matrix(Yk_test, Yk_pred)\n",
    "accuracy = accuracy_score(Yk_test, Yk_pred)\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm,'\\n')\n",
    "print('Normalized confusion matrix')\n",
    "print(cm_norm,'\\n')\n",
    "print('Accuracy of classification {:f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
