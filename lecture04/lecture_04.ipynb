{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 04 -- Starting a data project\n",
    "* Learning the business case\n",
    "* Define a MVP\n",
    "* Data collection\n",
    "* Data sanity checks, cleansing\n",
    "* Explorative analysis\n",
    "* Preparing the data for a simple model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example project -- Employee attrition model\n",
    "* \"I want to know why are my employees leaving and what can I do to mitigate that?\n",
    "* What kind of data we would be exploring?\n",
    "* Where does this data coming from?\n",
    "* What type of problem is this? Classification? Regression?\n",
    "* How can we structure easily attainable validation proof-of-concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://community.ibm.com/community/user/datascience/blogs/archive-user/2016/10/31/unlock-the-secrets-to-employee-retention-with-predictive-analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The costs of hiring/losing an employee\n",
    "\n",
    "# The cost of hiring a new employee, including the advertising, interviewing, screening, and hiring.\n",
    "# On-boarding a new worker, including training and management time.\n",
    "# Lost productivity, because it may take a new employee 1-2 years to reach the productivity of an existing one.\n",
    "# Lost engagement as other employees who see high turnover tend to disengage and lose productivity.\n",
    "# Training costs, which can add up to as 10-20 percent of an employee's salary or more in training for the first several years.\n",
    "# Cultural impact, as whenever someone leaves other employees will ask \"Why?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding the source of the data\n",
    "\n",
    "# ERP (enterprise resource planning)\n",
    "# What/who manages the data\n",
    "# Governed manually or by some process?\n",
    "# Mix of multiple systems, legacy solutions?\n",
    "# Shit in, shit out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classification problem\n",
    "\n",
    "# Employee terminated = boolean label\n",
    "# Estimation of various factors\n",
    "# Prediction of attrition with some confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple proof-of-concept\n",
    "\n",
    "# Linear model to estimate potential factors\n",
    "# Which factors are actionable or make sense in the business context?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start exploring the data in Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Notebooks\n",
    "\n",
    "![sci_paper.png](pics/sci_paper.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://www.theatlantic.com/science/archive/2018/04/the-scientific-paper-is-obsolete/556676/\n",
    "* Interactive, original idea based on Mathematica's Notebooks\n",
    "* Probably best environment for playing around\n",
    "* Very weak for version control and some serious collaboration\n",
    "* For anything more than a prototype or lecture purpose, switch to proper coding environment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jupyterlab.png](pics/jupyterlab.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-on Demo\n",
    "Play with [demo-notebook.ipynb](demo-notebook.ipynb)\n",
    "* Are you able to open the notebook?\n",
    "* Are you able to run/edit some cell and see the response?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternatives\n",
    "* Google Colab (https://colab.research.google.com/)\n",
    "* Deepnote (https://deepnote.com/dashboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's import core libraries and the data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# pandas is THE python library when dealing with tabular data\n",
    "import os\n",
    "# standard set of OS-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import <module> -- whole module\n",
    "# from <module> import <objects> -- separate objects, can be referenced without module name\n",
    "# import <module> as <alias> -- aliasing for simplicity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.listdir()\n",
    "# basicaly \"ls\" bash command of python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check the content of the \"data\" folder\n",
    "os.listdir(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# that's what we want to import\n",
    "#dataFile = \"data/WA_Fn-UseC_-HR-Employee-Attrition.csv\"\n",
    "# insert your path to the data\n",
    "dataFile = \"data/WA_Fn-UseC_-HR-Employee-Attrition.csv\"\n",
    "empData = pd.read_csv(dataFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(empData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check if we imported the file correctly, displaying first 100 rows\n",
    "empData.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empData.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data from other places aka detour to DB + APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequent routes to getting data are:\n",
    "* DBs -- usually, you'll be on the receiving end of analytical DBs. Basic SQL is pretty good skill to have.\n",
    "* Cloud storage -- AWS S3 and others. Again, ultra basic skill with cloud services goes a long way in your data science or ML career.\n",
    "* APIs -- REST or GraphQL. A standard how services/apps communicate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connecting to a public database\n",
    "* let's try this one \n",
    "    * https://rnacentral.org/help/public-database\n",
    "    * PostgreSQL is one of the most common one, easy to use via dedicated DB admin tools (DBeaver, PGAdmin, CLI,...) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "# let's define a connection\n",
    "conn = psycopg2.connect(\n",
    "    host=\"hh-pgsql-public.ebi.ac.uk\",\n",
    "    database=\"pfmegrnargs\",\n",
    "    user=\"reader\",\n",
    "    password=\"NWDMCE5xdipIjRrp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a cursor object using the cursor() method\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# sending a test query\n",
    "cursor.execute(\"select version()\")\n",
    "\n",
    "# fetching one row of data and print for validation\n",
    "data = cursor.fetchone()\n",
    "print(\"Connection established to: \",data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example query\n",
    "\n",
    "exampleQuery = \"\"\"\n",
    "SELECT\n",
    "  upi,     -- RNAcentral URS identifier\n",
    "  taxid,   -- NCBI taxid\n",
    "  ac       -- external accession\n",
    "FROM xref\n",
    "WHERE ac IN ('OTTHUMT00000106564.1', 'OTTHUMT00000416802.1')\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(exampleQuery)\n",
    "data = cursor.fetchall() #fetchall()\n",
    "RNAdata = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNAdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples to elaborate:\n",
    "* https://www.postman.com/api-evangelist/workspace/strava/request/35240-0bdb8a2d-f758-4d0f-9fb2-befe9b7d81c5?ctx=code\n",
    "* https://developers.strava.com/docs/getting-started/#curl\n",
    "* https://www.strava.com/settings/api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to ping Strava API\n",
    "import requests\n",
    "# just prettifying response \n",
    "import json\n",
    "\n",
    "url = \"https://www.strava.com/api/v3/athlete/\"\n",
    "\n",
    "payload={}\n",
    "headers = {\n",
    "  'Authorization': 'Bearer 78f4cbc498ab7b7f276ec7c8376cc1a7a4d4ac71'\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "json.loads(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Athlete stats\n",
    "url = \"https://www.strava.com/api/v3/athletes/16612601/stats\"\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "json.loads(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/docs/api#getting-started-installation-&-authentication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint #1\n",
    "* [ ] I imported the pandas and os libraries\n",
    "* [ ] I downloaded the data into a folder to which I can navigate from Jupyter\n",
    "* [ ] I imported the data as a pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's dig deeper into the basics of working with pandas dataframe (df ~ table, np.array ~ matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing the data by columns\n",
    "# dataFrame[column_name] or dataFrame[list_of_columns]\n",
    "empData['Attrition'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing the data, labels-based\n",
    "# dataFrame.loc[row_names, columns_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's select first 10 rows and first column\n",
    "empData.loc[1:10, 'Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now first 2 columns\n",
    "empData.loc[1:10, 'Age', 'Attrition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll have to put them into a list of fields\n",
    "empData.loc[1:10, ['Age', 'Attrition']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different kind of slicing, through integer-based position\n",
    "empData.iloc[1:10,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strange, huh? What about this?\n",
    "empData.iloc[1:10,1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last slice\n",
    "empData.iloc[1:10,1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing starts from 0 of course!\n",
    "# not-inclusive, so [1:3] kind-of means [2,3,4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing by maps (condition-based slicing)\n",
    "# let's create a map for all terminated employees\n",
    "terms = (empData['Attrition'] == 'Yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empData.loc[terms,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can slice it by this\n",
    "(empData.loc[terms,:]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chaining conditions, let's say terminated employees in Sales\n",
    "termsSales = ((terms) & (empData['Department'] == 'Sales'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empData.loc[termsSales,:].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding columns to a dataFrame\n",
    "empData['myNewColumn'] = 'You look nice today!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D data with axis!\n",
    "type(empData['Department'] == 'Sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(empData['Department'] == 'Sales').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on some condition\n",
    "empData['worksForSales'] = (empData['Department'] == 'Sales').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint #2\n",
    "* [ ] I can select three arbitrary columns from the data\n",
    "* [ ] I can add a new field which will label frequent travelers with medical background\n",
    "* [ ] I can return the data into the original shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empData['frequentMedicals'] = ((empData['BusinessTravel'] == 'Travel_Frequently') & \n",
    "                              (empData['EducationField'] == 'Medical')).values\n",
    "empData = empData.iloc[:,0:35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Profiling (get some sense of the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of the data\n",
    "empData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# datatypes\n",
    "empData.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get header\n",
    "empData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic description\n",
    "empData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupings\n",
    "# let's create a sensible subselect first\n",
    "empData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# what about monthly income by age?\n",
    "empData.loc[:,['Age', 'MonthlyIncome', 'JobSatisfaction']].groupby(['Age']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with NaNs, nulls or missing values\n",
    "empData['Age'].isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is there any?\n",
    "empData['Age'].isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what about detecting nulls in any column?\n",
    "empData.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what about detecting nulls in any column?\n",
    "empData.isnull().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empData.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# replace NaNs\n",
    "empData.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directly from df/series\n",
    "empData['Age'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empData.hist(figsize=(20,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic linechart\n",
    "empData.loc[:,['Age', 'MonthlyIncome']].groupby(['Age']).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring more interactivity with Plotly\n",
    "# \"Cufflinks binds Plotly directly to pandas dataframes.\"\n",
    "import cufflinks as cf\n",
    "cf.go_offline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empData.loc[:,['MonthlyIncome']].iplot(kind='histogram', bins = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empData.loc[:,['Age', 'MonthlyIncome']].groupby(['Age']).mean().iplot(kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "import seaborn as sn\n",
    "corrMatrix = empData.corr()\n",
    "plt.figure(figsize=(25, 20))\n",
    "sn.heatmap(corrMatrix, annot=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try some exploration of potential factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm, skew\n",
    "from scipy import stats\n",
    "import plotly.figure_factory as ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to estimate average and standard deviation by normal distribution\n",
    "(mu, sigma) = norm.fit(empData.loc[empData['Attrition'] == 'Yes', 'Age'])\n",
    "print(\n",
    "    'Ex-exmployees: average age = {:.1f} years old and standard deviation = {:.1f}'.format(mu, sigma))\n",
    "(mu, sigma) = norm.fit(empData.loc[empData['Attrition'] == 'No', 'Age'])\n",
    "print(\n",
    "    'Current exmployees: average age = {:.1f} years old and standard deviation = {:.1f}'.format(mu, sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again, let's do the same \n",
    "x1 = empData.loc[empData['Attrition'] == 'No', 'Age']\n",
    "x2 = empData.loc[empData['Attrition'] == 'Yes', 'Age']\n",
    "# Group data together\n",
    "hist_data = [x1, x2]\n",
    "group_labels = ['Active Employees', 'Ex-Employees']\n",
    "# Create distplot with custom bin_size\n",
    "fig = ff.create_distplot(hist_data, group_labels,\n",
    "                         curve_type='kde', show_hist=False, show_rug=False)\n",
    "# Add title\n",
    "fig['layout'].update(title='Age Distribution in Percent by Attrition Status')\n",
    "fig['layout'].update(xaxis=dict(range=[15, 60], dtick=5))\n",
    "# Plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seems like an interesting variable to explore, let's try to visualise that\n",
    "empData.groupby(['OverTime','Attrition']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empDataOverTime = pd.DataFrame(columns=[\"OverTime\", \"% of Leavers\"])\n",
    "i=0\n",
    "for field in list(empData['OverTime'].unique()):\n",
    "    ratio = empData[(empData['OverTime']==field)&(empData['Attrition']==\"Yes\")].shape[0] / empData[empData['OverTime']==field].shape[0]\n",
    "    empDataOverTime.loc[i] = (field, ratio*100)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empDataOverTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empDataOverTime.groupby('OverTime').sum().iplot(kind='bar', title = 'Leavers by OverTime (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling\n",
    "Getting some sense of the data auto-magically, let's look at two usefull libraries\n",
    "* Pandas-profiling (https://pypi.org/project/pandas-profiling/), (https://pandas-profiling.github.io/pandas-profiling/docs/master/rtd/pages/great_expectations_integration.html)\n",
    "* Autoviz (https://github.com/AutoViML/AutoViz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "profile = ProfileReport(empData, title=\"Employee Attrition Profile report\")\n",
    "\n",
    "# save to a local .html\n",
    "profile.to_file(\"jak_to_dopadlo.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoviz.AutoViz_Class import AutoViz_Class\n",
    "\n",
    "AV = AutoViz_Class()\n",
    "\n",
    "filename = \"\"\n",
    "sep = \",\"\n",
    "dft = AV.AutoViz(\n",
    "    \"\",\n",
    "    sep=\",\",\n",
    "    depVar=\"\",\n",
    "    dfte=empData,\n",
    "    header=0,\n",
    "    verbose=0,\n",
    "    lowess=False,\n",
    "    chart_format=\"Bokeh\",\n",
    "    max_rows_analyzed=150000,\n",
    "    max_cols_analyzed=30,\n",
    "    save_plot_dir=None\n",
    ")\n",
    "\n",
    "# we can try to specify depVar to \"Attrition\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great Expectations lib\n",
    "We'll follow the guide from https://docs.greatexpectations.io/docs/tutorials/getting_started/tutorial_setup on our data, please refer to that page for expanded guidance.\n",
    "\n",
    "#### Install GE framework\n",
    "Run\n",
    "\n",
    "`pip3 install great_expectations`\n",
    "\n",
    "Check installed version\n",
    "\n",
    "`great_expectations -- version`\n",
    "\n",
    "This should return\n",
    "\n",
    "`great_expectations, version 0.15.26`\n",
    "\n",
    "#### Setting up a Data Context\n",
    "Data context is a project config basically.\n",
    "\n",
    "Run\n",
    "\n",
    "`great_expectations init`\n",
    "\n",
    "Did we get a sweet ASCII art? Cool! Let's connect to a datasource now\n",
    "\n",
    "`great_expectations datasource new`\n",
    "\n",
    "Select relevant options and input data path\n",
    "\n",
    "`data/`\n",
    "\n",
    "We should get a configurable new jupyter noteebook. We should specify a name and validate if we can see our test .csv.\n",
    "\n",
    "#### Setting up and Review Expectations\n",
    "Now we'll set-up the checks that will be performed on the datasource.\n",
    "\n",
    "Let's go back to CLI and run\n",
    "\n",
    "`great_expectations suite new`\n",
    "\n",
    "\n",
    "Let's select auto-magic way, specify the datasource and name our expectation suite. It'll open up a new notebook to configure the expectations. Let's just go through that.\n",
    "\n",
    "\n",
    "#### Setting up a Checkpoint\n",
    "We probably want to mess up the data a little bit, so let's just do that.\n",
    "\n",
    "Once we have that, let's run\n",
    "\n",
    "`great_expectations checkpoint new getting_started_checkpoint`\n",
    "\n",
    "to set-up a new run. Let's go over the new jupyter to review and run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "* [ ] What other factor(s) could be potentially significant?\n",
    "* [ ] Import scikit-learn package\n",
    "* [ ] Prepare the data for a simple linear model of one continuous response with and without quadratic term\n",
    "* [ ] Compare the models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
